---
title: "Topic Modeling in R: Latent Dirichlet Allocation"
author: "Data Science Analysis"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
  pdf_document:
    toc: true
  word_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 6, 
                      fig.align = "center")
```

# Introduction

This document demonstrates **Topic Modeling** using **Latent Dirichlet Allocation (LDA)** in R. We will analyze a synthetic dataset about Australia to discover hidden topics and patterns in the text data.

## What is Topic Modeling?

Topic modeling is an unsupervised machine learning technique that automatically discovers abstract topics in a collection of documents. It helps us understand the main themes and concepts present in large text corpora without prior knowledge of the content.

## Latent Dirichlet Allocation (LDA)

LDA is one of the most popular topic modeling algorithms. It assumes that:
- Each document is a mixture of topics
- Each topic is a mixture of words
- Documents are generated by sampling topics and then sampling words from those topics

# Dataset Overview

Our dataset consists of `r length(list.files("Australia-Dataset/", pattern = "\\.txt$"))` text files about different aspects of Australia:

```{r dataset-info, echo=FALSE}
# List the files in the dataset
files <- list.files("Australia-Dataset/", pattern = "\\.txt$", full.names = TRUE)
file_names <- list.files("Australia-Dataset/", pattern = "\\.txt$")

cat("Dataset files:\n")
for (i in seq_along(files)) {
  cat(i, ".", file_names[i], "\n")
}
```

# Data Loading and Preprocessing

```{r load-libraries}
# Load required libraries
library(tm)
library(topicmodels)
library(tidytext)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
```

```{r load-data}
# Load the Australia dataset
data_dir <- "Australia-Dataset/"

# Read all text files into a corpus
corpus <- VCorpus(DirSource(data_dir), readerControl = list(language = "en"))

# Display basic information
cat("Number of documents:", length(corpus), "\n")
cat("Document names:", names(corpus), "\n")
```

```{r preprocess}
# Preprocess the text
corpus_clean <- tm_map(corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, stemDocument)

# Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus_clean)

# Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.8)

# Display DTM information
cat("Document-Term Matrix dimensions:", dim(dtm), "\n")
cat("Sparsity:", round(sum(dtm == 0) / (nrow(dtm) * ncol(dtm)), 3), "\n")
```

# Word Frequency Analysis

Let's analyze the frequency of words in our dataset:

```{r word-frequency}
# Calculate word frequencies
word_freq <- colSums(as.matrix(dtm))
word_freq <- sort(word_freq, decreasing = TRUE)

# Convert to data frame
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq, stringsAsFactors = FALSE)

# Display top 20 most frequent words
cat("Top 20 most frequent words:\n")
print(head(word_freq_df, 20))
```

```{r word-frequency-plot, fig.cap="Top 15 Most Frequent Words in Australia Dataset"}
# Create word frequency plot
top_words <- head(word_freq_df, 15)
barplot(top_words$freq, names.arg = top_words$word, 
        main = "Top 15 Most Frequent Words in Australia Dataset",
        xlab = "Words", ylab = "Frequency", 
        col = "steelblue", las = 2)
```

# Word Cloud Visualization

```{r wordcloud, fig.cap="Word Cloud of Most Frequent Terms"}
# Set seed for reproducibility
set.seed(123)

# Generate word cloud
wordcloud(words = word_freq_df$word, freq = word_freq_df$freq, 
          min.freq = 1, max.words = 100, 
          random.order = FALSE, 
          colors = brewer.pal(8, "Dark2"),
          scale = c(3, 0.5))
```

# Topic Modeling with LDA

Now let's perform topic modeling using Latent Dirichlet Allocation:

```{r lda-model}
# Set the number of topics
k <- 3

# Fit the LDA model
set.seed(1234)
lda_model <- LDA(dtm, k = k, control = list(seed = 1234))

# Extract topics using tidytext
topics <- tidy(lda_model, matrix = "beta")

# Get top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

cat("Top terms for each topic:\n")
print(top_terms)
```

```{r topic-visualization, fig.cap="Top Terms for Each Topic"}
# Create topic visualization
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(title = "Top Terms for Each Topic",
       x = "Terms", y = "Beta (Probability)") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white", color = NA),
        panel.background = element_rect(fill = "white", color = NA))
```

# Document-Topic Analysis

Let's examine how documents are assigned to topics:

```{r document-topics}
# Get document-topic probabilities
doc_topics <- tidy(lda_model, matrix = "gamma")

# Display document-topic assignments
cat("Document-topic probabilities:\n")
print(doc_topics)
```

```{r document-topic-plot, fig.cap="Document-Topic Probabilities"}
# Create document-topic visualization
doc_topics %>%
  mutate(document = reorder(document, gamma)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(title = "Document-Topic Probabilities",
       x = "Topic", y = "Gamma (Probability)") +
  theme_minimal() +
  theme(plot.background = element_rect(fill = "white", color = NA),
        panel.background = element_rect(fill = "white", color = NA))
```

# Alternative LDA Implementations

Let's test different numbers of topics to find the optimal configuration:

```{r perplexity-analysis}
# Test different numbers of topics
k_values <- c(2, 3, 4, 5)
perplexity_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  set.seed(1234)
  lda_temp <- LDA(dtm, k = k_values[i], control = list(seed = 1234))
  perplexity_scores[i] <- perplexity(lda_temp, dtm)
  cat("K =", k_values[i], "Perplexity =", perplexity_scores[i], "\n")
}
```

```{r perplexity-plot, fig.cap="Perplexity vs Number of Topics"}
# Create perplexity plot
plot(k_values, perplexity_scores, type = "b", 
     main = "Perplexity vs Number of Topics",
     xlab = "Number of Topics (K)", ylab = "Perplexity",
     col = "steelblue", pch = 19)
grid()
```

# Results and Interpretation

## Topic Assignments

```{r topic-assignments}
# Display topic assignments for each document
cat("Topic assignments for each document:\n")
for (i in 1:length(corpus)) {
  doc_name <- names(corpus)[i]
  doc_topic_probs <- doc_topics[doc_topics$document == i, ]
  dominant_topic <- doc_topic_probs$topic[which.max(doc_topic_probs$gamma)]
  cat("Document", i, "(", doc_name, "): Topic", dominant_topic, 
      "(probability:", round(max(doc_topic_probs$gamma), 3), ")\n")
}
```

## Summary

```{r summary}
cat("=== TOPIC MODELING SUMMARY ===\n")
cat("Dataset: Australia Dataset\n")
cat("Number of documents:", length(corpus), "\n")
cat("Number of terms in DTM:", ncol(dtm), "\n")
cat("Number of topics (K):", k, "\n")
cat("Total word frequency:", sum(word_freq), "\n")
cat("Most frequent word:", word_freq_df$word[1], "(", word_freq_df$freq[1], "times)\n")
```

# Conclusion

This analysis demonstrates how to:

1. **Preprocess text data** for topic modeling
2. **Analyze word frequencies** to understand the vocabulary
3. **Apply LDA** to discover hidden topics
4. **Visualize results** using various plots
5. **Evaluate model performance** using perplexity

The topic modeling approach successfully identified distinct themes in our Australia dataset, revealing the main topics present across the documents. This technique is valuable for understanding large text corpora and discovering patterns that might not be immediately obvious through manual inspection.

# References

- [Text Mining with R](https://www.tidytextmining.com/topicmodeling)
- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.
- Silge, J., & Robinson, D. (2017). Text mining with R: A tidy approach. O'Reilly Media.
