{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd2cdeb",
   "metadata": {},
   "source": [
    "# OpenCV Feature Detection Tutorial\n",
    "\n",
    "This notebook explores feature detection techniques in OpenCV, including corner detection, keypoint detection, and contour analysis.\n",
    "\n",
    "## Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Corner Detection](#corners)\n",
    "3. [Keypoint Detection](#keypoints)\n",
    "4. [Contour Detection and Analysis](#contours)\n",
    "5. [Feature Matching](#matching)\n",
    "6. [Practical Applications](#applications)\n",
    "7. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10dc66",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation {#setup}\n",
    "\n",
    "First, let's import the necessary libraries and our custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install opencv-python numpy matplotlib\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Add our source directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from feature_detection import corner_detection, keypoint_detection, contour_detection\n",
    "from basic_operations import image_io, display\n",
    "from utils import visualization\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a3f33",
   "metadata": {},
   "source": [
    "# Create Sample Images for Feature Detection\n",
    "\n",
    "Let's create and load images with various features for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74561c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample image\n",
    "image_path = '../sample_images/original/demo_image.jpg'\n",
    "if os.path.exists(image_path):\n",
    "    image = image_io.load_image(image_path)\n",
    "else:\n",
    "    # Create a demo image with various features\n",
    "    image = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add rectangles (corners)\n",
    "    cv2.rectangle(image, (50, 50), (200, 150), (255, 0, 0), 3)\n",
    "    cv2.rectangle(image, (250, 50), (400, 150), (0, 255, 0), -1)\n",
    "    \n",
    "    # Add circles\n",
    "    cv2.circle(image, (500, 100), 50, (0, 0, 255), 3)\n",
    "    cv2.circle(image, (150, 250), 30, (255, 255, 0), -1)\n",
    "    \n",
    "    # Add lines (edges)\n",
    "    cv2.line(image, (50, 200), (550, 200), (255, 255, 255), 3)\n",
    "    cv2.line(image, (50, 300), (550, 350), (128, 128, 128), 3)\n",
    "    \n",
    "    # Add a triangle\n",
    "    triangle_pts = np.array([[300, 250], [400, 250], [350, 350]], np.int32)\n",
    "    cv2.fillPoly(image, [triangle_pts], (255, 128, 0))\n",
    "    \n",
    "    # Add some texture pattern\n",
    "    for i in range(10, 600, 20):\n",
    "        for j in range(10, 400, 20):\n",
    "            if (i + j) % 40 == 0:\n",
    "                cv2.circle(image, (i, j), 2, (200, 200, 200), -1)\n",
    "\n",
    "# Convert to grayscale for some operations\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the test image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Color Test Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gray_image, cmap='gray')\n",
    "axes[1].set_title('Grayscale Test Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Grayscale shape: {gray_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93fdc1",
   "metadata": {},
   "source": [
    "## 2. Corner Detection {#corners}\n",
    "\n",
    "Corner detection identifies points where two edges meet, which are important features for image analysis.\n",
    "\n",
    "### 2.1 Harris Corner Detection\n",
    "The Harris corner detector finds corners by analyzing the gradient structure of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Harris corner detection\n",
    "harris_corners = corner_detection.harris_corner_detection(\n",
    "    gray_image, \n",
    "    block_size=2, \n",
    "    ksize=3, \n",
    "    k=0.04,\n",
    "    threshold=0.01\n",
    ")\n",
    "\n",
    "# Harris corner response\n",
    "harris_response = cv2.cornerHarris(gray_image, 2, 3, 0.04)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Harris response\n",
    "axes[1].imshow(harris_response, cmap='hot')\n",
    "axes[1].set_title('Harris Corner Response')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Detected corners\n",
    "image_with_harris = image.copy()\n",
    "image_with_harris[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]\n",
    "axes[2].imshow(cv2.cvtColor(image_with_harris, cv2.COLOR_BGR2RGB))\n",
    "axes[2].set_title('Harris Corners Detected (Red)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Harris corners detected: {np.sum(harris_corners > 0.01 * harris_corners.max())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416319a0",
   "metadata": {},
   "source": [
    "### 2.2 Shi-Tomasi Corner Detection\n",
    "Good Features to Track algorithm, which is an improvement over Harris corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Shi-Tomasi corner detection\n",
    "corners_shi_tomasi = corner_detection.shi_tomasi_corner_detection(\n",
    "    gray_image,\n",
    "    max_corners=100,\n",
    "    quality_level=0.01,\n",
    "    min_distance=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Shi-Tomasi corners\n",
    "image_with_shi_tomasi = image.copy()\n",
    "if corners_shi_tomasi is not None:\n",
    "    for corner in corners_shi_tomasi:\n",
    "        x, y = corner.ravel()\n",
    "        cv2.circle(image_with_shi_tomasi, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(image_with_shi_tomasi, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'Shi-Tomasi Corners (Green) - {len(corners_shi_tomasi) if corners_shi_tomasi is not None else 0} detected')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b5204",
   "metadata": {},
   "source": [
    "### 2.3 FAST Corner Detection\n",
    "Features from Accelerated Segment Test - a high-speed corner detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FAST corner detection\n",
    "fast_keypoints = corner_detection.fast_corner_detection(\n",
    "    gray_image,\n",
    "    threshold=50,\n",
    "    non_max_suppression=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# FAST corners\n",
    "image_with_fast = image.copy()\n",
    "for kp in fast_keypoints:\n",
    "    x, y = int(kp.pt[0]), int(kp.pt[1])\n",
    "    cv2.circle(image_with_fast, (x, y), 3, (255, 0, 255), -1)\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(image_with_fast, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'FAST Corners (Magenta) - {len(fast_keypoints)} detected')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea5047",
   "metadata": {},
   "source": [
    "### 2.4 Comparison of Corner Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all corner detection methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Harris\n",
    "axes[0, 1].imshow(cv2.cvtColor(image_with_harris, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title(f'Harris Corners (Red)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Shi-Tomasi\n",
    "axes[1, 0].imshow(cv2.cvtColor(image_with_shi_tomasi, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title(f'Shi-Tomasi Corners (Green)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# FAST\n",
    "axes[1, 1].imshow(cv2.cvtColor(image_with_fast, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title(f'FAST Corners (Magenta)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"Corner Detection Comparison:\")\n",
    "print(f\"Harris: {np.sum(harris_corners > 0.01 * harris_corners.max())} corners\")\n",
    "print(f\"Shi-Tomasi: {len(corners_shi_tomasi) if corners_shi_tomasi is not None else 0} corners\")\n",
    "print(f\"FAST: {len(fast_keypoints)} corners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a251a",
   "metadata": {},
   "source": [
    "## 3. Keypoint Detection {#keypoints}\n",
    "\n",
    "Keypoint detectors find distinctive points that can be reliably identified across different images.\n",
    "\n",
    "### 3.1 SIFT (Scale-Invariant Feature Transform)\n",
    "Detects keypoints that are invariant to scale, rotation, and partially invariant to illumination changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb499f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SIFT keypoint detection\n",
    "try:\n",
    "    sift_keypoints, sift_descriptors = keypoint_detection.sift_detection(gray_image)\n",
    "    \n",
    "    # Draw SIFT keypoints\n",
    "    image_with_sift = cv2.drawKeypoints(\n",
    "        image, sift_keypoints, None, \n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(image_with_sift, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'SIFT Keypoints - {len(sift_keypoints)} detected')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"SIFT keypoints detected: {len(sift_keypoints)}\")\n",
    "    print(f\"SIFT descriptor shape: {sift_descriptors.shape if sift_descriptors is not None else 'None'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SIFT detection failed: {e}\")\n",
    "    print(\"Note: SIFT might not be available in your OpenCV build\")\n",
    "    sift_keypoints, sift_descriptors = [], None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c86c3",
   "metadata": {},
   "source": [
    "### 3.2 ORB (Oriented FAST and Rotated BRIEF)\n",
    "A free alternative to SIFT/SURF that's fast and works well for many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ORB keypoint detection\n",
    "orb_keypoints, orb_descriptors = keypoint_detection.orb_detection(\n",
    "    gray_image, \n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "# Draw ORB keypoints\n",
    "image_with_orb = cv2.drawKeypoints(\n",
    "    image, orb_keypoints, None, \n",
    "    color=(0, 255, 0), \n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(image_with_orb, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'ORB Keypoints - {len(orb_keypoints)} detected')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ORB keypoints detected: {len(orb_keypoints)}\")\n",
    "print(f\"ORB descriptor shape: {orb_descriptors.shape if orb_descriptors is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176e708",
   "metadata": {},
   "source": [
    "### 3.3 AKAZE Keypoint Detection\n",
    "Another scale and rotation invariant detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb782f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply AKAZE keypoint detection\n",
    "try:\n",
    "    akaze_keypoints, akaze_descriptors = keypoint_detection.akaze_detection(gray_image)\n",
    "    \n",
    "    # Draw AKAZE keypoints\n",
    "    image_with_akaze = cv2.drawKeypoints(\n",
    "        image, akaze_keypoints, None, \n",
    "        color=(255, 0, 0), \n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(image_with_akaze, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'AKAZE Keypoints - {len(akaze_keypoints)} detected')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"AKAZE keypoints detected: {len(akaze_keypoints)}\")\n",
    "    print(f\"AKAZE descriptor shape: {akaze_descriptors.shape if akaze_descriptors is not None else 'None'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"AKAZE detection failed: {e}\")\n",
    "    akaze_keypoints, akaze_descriptors = [], None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1902a076",
   "metadata": {},
   "source": [
    "## 4. Contour Detection and Analysis {#contours}\n",
    "\n",
    "Contours are curves joining continuous points having the same color or intensity.\n",
    "\n",
    "### 4.1 Finding Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary image for contour detection\n",
    "# Apply threshold to get binary image\n",
    "_, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Find contours\n",
    "contours = contour_detection.find_contours(binary_image)\n",
    "\n",
    "# Draw contours\n",
    "image_with_contours = image.copy()\n",
    "cv2.drawContours(image_with_contours, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(binary_image, cmap='gray')\n",
    "axes[1].set_title('Binary Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(cv2.cvtColor(image_with_contours, cv2.COLOR_BGR2RGB))\n",
    "axes[2].set_title(f'Contours Detected - {len(contours)} contours')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of contours found: {len(contours)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba6cbc",
   "metadata": {},
   "source": [
    "### 4.2 Contour Analysis\n",
    "Analyzing contour properties like area, perimeter, and shape characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze contours\n",
    "if contours:\n",
    "    # Get the largest contours\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n",
    "    \n",
    "    # Analyze each contour\n",
    "    contour_analysis = []\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "    \n",
    "    analysis_image = image.copy()\n",
    "    \n",
    "    for i, contour in enumerate(largest_contours):\n",
    "        # Calculate contour properties\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        \n",
    "        # Approximate contour\n",
    "        epsilon = 0.02 * perimeter\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        # Calculate bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Calculate aspect ratio\n",
    "        aspect_ratio = float(w) / h\n",
    "        \n",
    "        # Calculate extent (ratio of contour area to bounding rectangle area)\n",
    "        rect_area = w * h\n",
    "        extent = float(area) / rect_area\n",
    "        \n",
    "        # Calculate solidity (ratio of contour area to convex hull area)\n",
    "        hull = cv2.convexHull(contour)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area) / hull_area if hull_area > 0 else 0\n",
    "        \n",
    "        contour_analysis.append({\n",
    "            'contour_id': i,\n",
    "            'area': area,\n",
    "            'perimeter': perimeter,\n",
    "            'vertices': len(approx),\n",
    "            'aspect_ratio': aspect_ratio,\n",
    "            'extent': extent,\n",
    "            'solidity': solidity\n",
    "        })\n",
    "        \n",
    "        # Draw contour and bounding rectangle\n",
    "        cv2.drawContours(analysis_image, [contour], -1, colors[i], 2)\n",
    "        cv2.rectangle(analysis_image, (x, y), (x + w, y + h), colors[i], 2)\n",
    "        \n",
    "        # Draw convex hull\n",
    "        cv2.drawContours(analysis_image, [hull], -1, colors[i], 1)\n",
    "        \n",
    "        # Label the contour\n",
    "        cv2.putText(analysis_image, f'C{i}', (x, y-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[i], 2)\n",
    "    \n",
    "    # Display analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(analysis_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title('Contour Analysis\\n(Contours + Bounding Boxes + Convex Hulls)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(\"\\nContour Analysis Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ID':<3} {'Area':<8} {'Perimeter':<10} {'Vertices':<8} {'Aspect':<7} {'Extent':<7} {'Solidity':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for analysis in contour_analysis:\n",
    "        print(f\"{analysis['contour_id']:<3} \"\n",
    "              f\"{analysis['area']:<8.1f} \"\n",
    "              f\"{analysis['perimeter']:<10.1f} \"\n",
    "              f\"{analysis['vertices']:<8} \"\n",
    "              f\"{analysis['aspect_ratio']:<7.2f} \"\n",
    "              f\"{analysis['extent']:<7.2f} \"\n",
    "              f\"{analysis['solidity']:<8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740605a",
   "metadata": {},
   "source": [
    "### 4.3 Shape Matching\n",
    "Comparing contours to identify similar shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape matching example\n",
    "if len(contours) >= 2:\n",
    "    # Select reference contour (first large contour)\n",
    "    reference_contour = largest_contours[0]\n",
    "    \n",
    "    # Calculate shape distances to reference\n",
    "    shape_distances = []\n",
    "    match_image = image.copy()\n",
    "    \n",
    "    # Draw reference contour in red\n",
    "    cv2.drawContours(match_image, [reference_contour], -1, (0, 0, 255), 3)\n",
    "    \n",
    "    for i, contour in enumerate(largest_contours[1:], 1):\n",
    "        # Calculate Hu moments for shape matching\n",
    "        distance = contour_detection.match_shapes(reference_contour, contour)\n",
    "        shape_distances.append((i, distance))\n",
    "        \n",
    "        # Color code based on similarity (green = similar, blue = different)\n",
    "        if distance < 0.1:\n",
    "            color = (0, 255, 0)  # Green - very similar\n",
    "        elif distance < 0.3:\n",
    "            color = (255, 255, 0)  # Yellow - somewhat similar\n",
    "        else:\n",
    "            color = (255, 0, 0)  # Blue - different\n",
    "        \n",
    "        cv2.drawContours(match_image, [contour], -1, color, 2)\n",
    "        \n",
    "        # Add distance label\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "            cv2.putText(match_image, f'{distance:.3f}', (cx-20, cy), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Display shape matching results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(match_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title('Shape Matching\\n(Red=Reference, Green=Similar, Yellow=Somewhat, Blue=Different)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print shape matching results\n",
    "    print(\"\\nShape Matching Results:\")\n",
    "    print(f\"Reference contour: Contour 0 (Red)\")\n",
    "    print(\"Distance < 0.1: Very similar (Green)\")\n",
    "    print(\"Distance < 0.3: Somewhat similar (Yellow)\")\n",
    "    print(\"Distance >= 0.3: Different (Blue)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for contour_id, distance in shape_distances:\n",
    "        similarity = \"Very similar\" if distance < 0.1 else \"Somewhat similar\" if distance < 0.3 else \"Different\"\n",
    "        print(f\"Contour {contour_id}: Distance = {distance:.4f} ({similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a9669",
   "metadata": {},
   "source": [
    "## 5. Feature Matching {#matching}\n",
    "\n",
    "Matching features between different images or different views of the same scene.\n",
    "\n",
    "### 5.1 ORB Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second image by rotating and scaling the first\n",
    "height, width = image.shape[:2]\n",
    "center = (width // 2, height // 2)\n",
    "\n",
    "# Rotation and scaling transformation\n",
    "M = cv2.getRotationMatrix2D(center, 15, 0.8)\n",
    "image2 = cv2.warpAffine(image, M, (width, height))\n",
    "\n",
    "# Add some noise to make matching more challenging\n",
    "noise = np.random.normal(0, 10, image2.shape).astype(np.uint8)\n",
    "image2 = cv2.add(image2, noise)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray1 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect ORB features in both images\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "kp1, des1 = orb.detectAndCompute(gray1, None)\n",
    "kp2, des2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "# Match features\n",
    "if des1 is not None and des2 is not None:\n",
    "    # Create BF matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    # Draw matches\n",
    "    match_image = cv2.drawMatches(\n",
    "        image, kp1, image2, kp2, matches[:50], None, \n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Show both images\n",
    "    combined_images = np.hstack([\n",
    "        cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "        cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    ])\n",
    "    axes[0].imshow(combined_images)\n",
    "    axes[0].set_title(f'Original Image (left) vs Transformed Image (right)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show matches\n",
    "    axes[1].imshow(cv2.cvtColor(match_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'ORB Feature Matches - {len(matches)} total, showing best 50')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Keypoints in image 1: {len(kp1)}\")\n",
    "    print(f\"Keypoints in image 2: {len(kp2)}\")\n",
    "    print(f\"Good matches found: {len(matches)}\")\n",
    "    \n",
    "    # Print some match statistics\n",
    "    distances = [m.distance for m in matches[:20]]\n",
    "    print(f\"Best 20 match distances: {distances}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No descriptors found for matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8d82",
   "metadata": {},
   "source": [
    "### 5.2 Robust Feature Matching with RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust matching using RANSAC to find homography\n",
    "if des1 is not None and des2 is not None and len(matches) > 10:\n",
    "    # Extract matched points\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Find homography using RANSAC\n",
    "    M_homography, mask = cv2.findHomography(src_pts, dst_pts, \n",
    "                                          cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # Separate inliers and outliers\n",
    "    inliers = [matches[i] for i in range(len(matches)) if mask[i]]\n",
    "    outliers = [matches[i] for i in range(len(matches)) if not mask[i]]\n",
    "    \n",
    "    # Draw only inlier matches\n",
    "    inlier_match_image = cv2.drawMatches(\n",
    "        image, kp1, image2, kp2, inliers, None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    \n",
    "    # Draw the found homography\n",
    "    if M_homography is not None:\n",
    "        h, w = gray1.shape\n",
    "        corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "        transformed_corners = cv2.perspectiveTransform(corners, M_homography)\n",
    "        \n",
    "        # Draw the transformed quadrilateral on image2\n",
    "        image2_with_quad = image2.copy()\n",
    "        cv2.polylines(image2_with_quad, [np.int32(transformed_corners)], True, (0, 255, 0), 3)\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
    "    \n",
    "    # All matches\n",
    "    axes[0].imshow(cv2.cvtColor(match_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f'All ORB Matches - {len(matches)} matches')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Inlier matches only\n",
    "    axes[1].imshow(cv2.cvtColor(inlier_match_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'RANSAC Inlier Matches - {len(inliers)} inliers, {len(outliers)} outliers')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Homography result\n",
    "    if M_homography is not None:\n",
    "        axes[2].imshow(cv2.cvtColor(image2_with_quad, cv2.COLOR_BGR2RGB))\n",
    "        axes[2].set_title('Detected Object (Green Quadrilateral)')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total matches: {len(matches)}\")\n",
    "    print(f\"Inlier matches: {len(inliers)}\")\n",
    "    print(f\"Outlier matches: {len(outliers)}\")\n",
    "    print(f\"Inlier ratio: {len(inliers)/len(matches):.2%}\")\n",
    "    \n",
    "    if M_homography is not None:\n",
    "        print(f\"\\nHomography matrix found:\")\n",
    "        print(M_homography)\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough matches for homography estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7657c",
   "metadata": {},
   "source": [
    "## 6. Practical Applications {#applications}\n",
    "\n",
    "### 6.1 Object Detection Using Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template matching example\n",
    "# Extract a template from the original image\n",
    "template_x, template_y, template_w, template_h = 250, 50, 150, 100\n",
    "template = image[template_y:template_y+template_h, template_x:template_x+template_w]\n",
    "\n",
    "# Search for template in the transformed image\n",
    "gray_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "result = cv2.matchTemplate(gray2, gray_template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Find best match location\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0] + template_w, top_left[1] + template_h)\n",
    "\n",
    "# Draw rectangle around detected region\n",
    "image2_with_detection = image2.copy()\n",
    "cv2.rectangle(image2_with_detection, top_left, bottom_right, (0, 255, 0), 3)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Template\n",
    "axes[0, 0].imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Template')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Template matching result\n",
    "axes[0, 1].imshow(result, cmap='hot')\n",
    "axes[0, 1].set_title(f'Template Matching Result\\nBest match: {max_val:.3f}')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Original with template location\n",
    "image_with_template_marked = image.copy()\n",
    "cv2.rectangle(image_with_template_marked, (template_x, template_y), \n",
    "             (template_x + template_w, template_y + template_h), (255, 0, 0), 3)\n",
    "axes[1, 0].imshow(cv2.cvtColor(image_with_template_marked, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title('Original with Template Location (Blue)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Detection result\n",
    "axes[1, 1].imshow(cv2.cvtColor(image2_with_detection, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title('Template Detected (Green)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Template matching confidence: {max_val:.3f}\")\n",
    "print(f\"Detection location: {top_left}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc986f66",
   "metadata": {},
   "source": [
    "### 6.2 Feature-based Object Tracking\n",
    "Simulating object tracking using feature matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate object tracking across multiple frames\n",
    "def create_simulated_frames(base_image, n_frames=5):\n",
    "    \"\"\"Create simulated video frames with moving/rotating objects.\"\"\"\n",
    "    frames = []\n",
    "    height, width = base_image.shape[:2]\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        # Apply different transformations to simulate movement\n",
    "        angle = i * 5  # Rotation\n",
    "        scale = 1.0 + i * 0.05  # Scaling\n",
    "        tx = i * 10  # Translation X\n",
    "        ty = i * 5   # Translation Y\n",
    "        \n",
    "        # Create transformation matrix\n",
    "        center = (width // 2, height // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "        M[0, 2] += tx\n",
    "        M[1, 2] += ty\n",
    "        \n",
    "        # Apply transformation\n",
    "        transformed = cv2.warpAffine(base_image, M, (width, height))\n",
    "        \n",
    "        # Add some noise\n",
    "        noise = np.random.normal(0, 5, transformed.shape).astype(np.uint8)\n",
    "        transformed = cv2.add(transformed, noise)\n",
    "        \n",
    "        frames.append(transformed)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Create simulated frames\n",
    "frames = create_simulated_frames(image, 4)\n",
    "\n",
    "# Track features across frames\n",
    "orb = cv2.ORB_create(nfeatures=200)\n",
    "\n",
    "# Detect features in first frame\n",
    "gray_first = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "kp_first, des_first = orb.detectAndCompute(gray_first, None)\n",
    "\n",
    "# Track across subsequent frames\n",
    "tracking_results = []\n",
    "\n",
    "for i, frame in enumerate(frames[1:], 1):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = orb.detectAndCompute(gray_frame, None)\n",
    "    \n",
    "    if des_first is not None and des_frame is not None:\n",
    "        # Match features\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des_first, des_frame)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "        \n",
    "        # Keep good matches\n",
    "        good_matches = [m for m in matches if m.distance < 30]\n",
    "        \n",
    "        tracking_results.append({\n",
    "            'frame': i,\n",
    "            'keypoints': len(kp_frame),\n",
    "            'matches': len(good_matches),\n",
    "            'match_image': cv2.drawMatches(\n",
    "                frames[0], kp_first, frame, kp_frame, \n",
    "                good_matches[:20], None,\n",
    "                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "            )\n",
    "        })\n",
    "\n",
    "# Display tracking results\n",
    "fig, axes = plt.subplots(len(tracking_results), 1, figsize=(15, 5 * len(tracking_results)))\n",
    "\n",
    "if len(tracking_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, result in enumerate(tracking_results):\n",
    "    axes[i].imshow(cv2.cvtColor(result['match_image'], cv2.COLOR_BGR2RGB))\n",
    "    axes[i].set_title(f\"Frame {result['frame']}: {result['keypoints']} keypoints, \"\n",
    "                     f\"{result['matches']} matches (showing best 20)\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print tracking statistics\n",
    "print(\"Feature Tracking Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Frame':<6} {'Keypoints':<10} {'Matches':<8}\")\n",
    "print(\"-\" * 50)\n",
    "for result in tracking_results:\n",
    "    print(f\"{result['frame']:<6} {result['keypoints']:<10} {result['matches']:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd7e59",
   "metadata": {},
   "source": [
    "## 7. Exercises {#exercises}\n",
    "\n",
    "Try these exercises to practice feature detection techniques:\n",
    "\n",
    "### Exercise 1: Multi-Scale Feature Detection\n",
    "Detect features at multiple scales and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Multi-scale feature detection\n",
    "def detect_features_at_scales(image, scales=[0.5, 1.0, 1.5, 2.0]):\n",
    "    \"\"\"Detect ORB features at multiple scales.\"\"\"\n",
    "    results = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Resize image\n",
    "        width = int(image.shape[1] * scale)\n",
    "        height = int(image.shape[0] * scale)\n",
    "        scaled_image = cv2.resize(gray, (width, height))\n",
    "        \n",
    "        # Detect features\n",
    "        orb = cv2.ORB_create(nfeatures=200)\n",
    "        keypoints, descriptors = orb.detectAndCompute(scaled_image, None)\n",
    "        \n",
    "        # Scale keypoints back to original size\n",
    "        scaled_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            scaled_kp = cv2.KeyPoint(\n",
    "                x=kp.pt[0] / scale,\n",
    "                y=kp.pt[1] / scale,\n",
    "                size=kp.size / scale,\n",
    "                angle=kp.angle,\n",
    "                response=kp.response,\n",
    "                octave=kp.octave,\n",
    "                class_id=kp.class_id\n",
    "            )\n",
    "            scaled_keypoints.append(scaled_kp)\n",
    "        \n",
    "        results.append({\n",
    "            'scale': scale,\n",
    "            'keypoints': scaled_keypoints,\n",
    "            'count': len(scaled_keypoints)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply multi-scale detection\n",
    "scale_results = detect_features_at_scales(image)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
    "\n",
    "for i, result in enumerate(scale_results):\n",
    "    if i < 4:  # Only show first 4 scales\n",
    "        # Draw keypoints\n",
    "        image_with_kp = image.copy()\n",
    "        for kp in result['keypoints']:\n",
    "            center = (int(kp.pt[0]), int(kp.pt[1]))\n",
    "            radius = max(1, int(kp.size / 4))\n",
    "            cv2.circle(image_with_kp, center, radius, colors[i], 2)\n",
    "        \n",
    "        axes[i].imshow(cv2.cvtColor(image_with_kp, cv2.COLOR_BGR2RGB))\n",
    "        axes[i].set_title(f\"Scale {result['scale']}: {result['count']} features\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print scale analysis\n",
    "print(\"Multi-Scale Feature Detection Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Scale':<8} {'Features':<10} {'Density':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for result in scale_results:\n",
    "    density = result['count'] / (image.shape[0] * image.shape[1] / 1000)  # per 1000 pixels\n",
    "    print(f\"{result['scale']:<8.1f} {result['count']:<10} {density:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566dc027",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Stability Analysis\n",
    "Analyze how stable features are across different transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab466af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Feature stability analysis\n",
    "def analyze_feature_stability(base_image, transformations):\n",
    "    \"\"\"Analyze feature stability across different transformations.\"\"\"\n",
    "    gray_base = cv2.cvtColor(base_image, cv2.COLOR_BGR2GRAY)\n",
    "    orb = cv2.ORB_create(nfeatures=300)\n",
    "    \n",
    "    # Detect features in base image\n",
    "    kp_base, des_base = orb.detectAndCompute(gray_base, None)\n",
    "    \n",
    "    stability_results = []\n",
    "    \n",
    "    for trans_name, trans_func in transformations.items():\n",
    "        # Apply transformation\n",
    "        transformed_image = trans_func(base_image)\n",
    "        gray_trans = cv2.cvtColor(transformed_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect features in transformed image\n",
    "        kp_trans, des_trans = orb.detectAndCompute(gray_trans, None)\n",
    "        \n",
    "        if des_base is not None and des_trans is not None:\n",
    "            # Match features\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "            matches = bf.match(des_base, des_trans)\n",
    "            \n",
    "            # Count good matches\n",
    "            good_matches = [m for m in matches if m.distance < 30]\n",
    "            \n",
    "            # Calculate stability metrics\n",
    "            stability_ratio = len(good_matches) / len(kp_base) if len(kp_base) > 0 else 0\n",
    "            \n",
    "            stability_results.append({\n",
    "                'transformation': trans_name,\n",
    "                'base_features': len(kp_base),\n",
    "                'trans_features': len(kp_trans),\n",
    "                'matches': len(good_matches),\n",
    "                'stability_ratio': stability_ratio,\n",
    "                'transformed_image': transformed_image\n",
    "            })\n",
    "    \n",
    "    return stability_results\n",
    "\n",
    "# Define transformations\n",
    "def rotation_15(img):\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), 15, 1.0)\n",
    "    return cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "def scale_80(img):\n",
    "    h, w = img.shape[:2]\n",
    "    return cv2.resize(img, (int(w*0.8), int(h*0.8)))\n",
    "\n",
    "def brightness_change(img):\n",
    "    return cv2.convertScaleAbs(img, alpha=1.2, beta=20)\n",
    "\n",
    "def blur_effect(img):\n",
    "    return cv2.GaussianBlur(img, (7, 7), 0)\n",
    "\n",
    "transformations = {\n",
    "    'Rotation 15°': rotation_15,\n",
    "    'Scale 80%': scale_80,\n",
    "    'Brightness +20%': brightness_change,\n",
    "    'Blur (7x7)': blur_effect\n",
    "}\n",
    "\n",
    "# Analyze stability\n",
    "stability_results = analyze_feature_stability(image, transformations)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show transformed images\n",
    "positions = [(0, 1), (0, 2), (1, 0), (1, 1)]\n",
    "for i, result in enumerate(stability_results):\n",
    "    if i < 4:\n",
    "        row, col = positions[i]\n",
    "        axes[row, col].imshow(cv2.cvtColor(result['transformed_image'], cv2.COLOR_BGR2RGB))\n",
    "        axes[row, col].set_title(f\"{result['transformation']}\\n\"\n",
    "                                f\"Stability: {result['stability_ratio']:.2%}\")\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "# Create stability chart\n",
    "axes[1, 2].bar([r['transformation'] for r in stability_results], \n",
    "               [r['stability_ratio'] for r in stability_results])\n",
    "axes[1, 2].set_title('Feature Stability Ratios')\n",
    "axes[1, 2].set_ylabel('Stability Ratio')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print stability analysis\n",
    "print(\"Feature Stability Analysis:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Transformation':<15} {'Base':<6} {'Trans':<6} {'Matches':<8} {'Stability':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result in stability_results:\n",
    "    print(f\"{result['transformation']:<15} \"\n",
    "          f\"{result['base_features']:<6} \"\n",
    "          f\"{result['trans_features']:<6} \"\n",
    "          f\"{result['matches']:<8} \"\n",
    "          f\"{result['stability_ratio']:<10.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faecb",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Feature Descriptor\n",
    "Create a simple custom feature descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec32582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Custom feature descriptor\n",
    "def simple_patch_descriptor(image, keypoint, patch_size=16):\n",
    "    \"\"\"Create a simple patch-based descriptor around a keypoint.\"\"\"\n",
    "    x, y = int(keypoint.pt[0]), int(keypoint.pt[1])\n",
    "    half_size = patch_size // 2\n",
    "    \n",
    "    # Check bounds\n",
    "    if (x - half_size < 0 or x + half_size >= image.shape[1] or \n",
    "        y - half_size < 0 or y + half_size >= image.shape[0]):\n",
    "        return None\n",
    "    \n",
    "    # Extract patch\n",
    "    patch = image[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "    \n",
    "    # Create descriptor from patch statistics\n",
    "    descriptor = np.array([\n",
    "        np.mean(patch),              # Mean intensity\n",
    "        np.std(patch),               # Standard deviation\n",
    "        np.mean(patch[:half_size, :]),  # Top half mean\n",
    "        np.mean(patch[half_size:, :]),  # Bottom half mean\n",
    "        np.mean(patch[:, :half_size]),  # Left half mean\n",
    "        np.mean(patch[:, half_size:]),  # Right half mean\n",
    "        np.mean(np.gradient(patch.astype(float))[0]),  # Gradient X\n",
    "        np.mean(np.gradient(patch.astype(float))[1]),  # Gradient Y\n",
    "    ])\n",
    "    \n",
    "    return descriptor\n",
    "\n",
    "def match_custom_descriptors(desc1, desc2, threshold=50):\n",
    "    \"\"\"Match custom descriptors using Euclidean distance.\"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    for i, d1 in enumerate(desc1):\n",
    "        if d1 is not None:\n",
    "            best_distance = float('inf')\n",
    "            best_match = -1\n",
    "            \n",
    "            for j, d2 in enumerate(desc2):\n",
    "                if d2 is not None:\n",
    "                    distance = np.linalg.norm(d1 - d2)\n",
    "                    if distance < best_distance:\n",
    "                        best_distance = distance\n",
    "                        best_match = j\n",
    "            \n",
    "            if best_distance < threshold:\n",
    "                matches.append((i, best_match, best_distance))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Detect corners for custom descriptor\n",
    "corners = cv2.goodFeaturesToTrack(gray_image, maxCorners=100, \n",
    "                                 qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "if corners is not None:\n",
    "    # Convert to keypoints\n",
    "    keypoints = [cv2.KeyPoint(x[0][0], x[0][1], 10) for x in corners]\n",
    "    \n",
    "    # Create custom descriptors\n",
    "    custom_descriptors = []\n",
    "    for kp in keypoints:\n",
    "        desc = simple_patch_descriptor(gray_image, kp)\n",
    "        custom_descriptors.append(desc)\n",
    "    \n",
    "    # Create transformed image for matching\n",
    "    h, w = image.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w//2, h//2), 10, 0.9)\n",
    "    image_transformed = cv2.warpAffine(image, M, (w, h))\n",
    "    gray_transformed = cv2.cvtColor(image_transformed, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect corners in transformed image\n",
    "    corners_trans = cv2.goodFeaturesToTrack(gray_transformed, maxCorners=100,\n",
    "                                           qualityLevel=0.01, minDistance=10)\n",
    "    \n",
    "    if corners_trans is not None:\n",
    "        # Convert to keypoints\n",
    "        keypoints_trans = [cv2.KeyPoint(x[0][0], x[0][1], 10) for x in corners_trans]\n",
    "        \n",
    "        # Create descriptors for transformed image\n",
    "        custom_descriptors_trans = []\n",
    "        for kp in keypoints_trans:\n",
    "            desc = simple_patch_descriptor(gray_transformed, kp)\n",
    "            custom_descriptors_trans.append(desc)\n",
    "        \n",
    "        # Match descriptors\n",
    "        matches = match_custom_descriptors(custom_descriptors, custom_descriptors_trans)\n",
    "        \n",
    "        # Visualize results\n",
    "        match_image = np.hstack([image, image_transformed])\n",
    "        \n",
    "        # Draw matches\n",
    "        for i, (idx1, idx2, distance) in enumerate(matches[:20]):  # Show first 20 matches\n",
    "            pt1 = keypoints[idx1].pt\n",
    "            pt2 = keypoints_trans[idx2].pt\n",
    "            \n",
    "            # Adjust pt2 for concatenated image\n",
    "            pt2_adjusted = (pt2[0] + w, pt2[1])\n",
    "            \n",
    "            # Draw line\n",
    "            cv2.line(match_image, \n",
    "                    (int(pt1[0]), int(pt1[1])), \n",
    "                    (int(pt2_adjusted[0]), int(pt2_adjusted[1])),\n",
    "                    (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw circles\n",
    "            cv2.circle(match_image, (int(pt1[0]), int(pt1[1])), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(match_image, (int(pt2_adjusted[0]), int(pt2_adjusted[1])), 5, (255, 0, 0), -1)\n",
    "        \n",
    "        # Display results\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(cv2.cvtColor(match_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Custom Descriptor Matching\\n{len(matches)} matches found (showing first 20)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Custom Feature Descriptor Results:\")\n",
    "        print(f\"Keypoints in image 1: {len(keypoints)}\")\n",
    "        print(f\"Keypoints in image 2: {len(keypoints_trans)}\")\n",
    "        print(f\"Custom matches found: {len(matches)}\")\n",
    "        \n",
    "        if matches:\n",
    "            distances = [m[2] for m in matches[:10]]\n",
    "            print(f\"Top 10 match distances: {distances}\")\n",
    "            print(f\"Average match distance: {np.mean([m[2] for m in matches]):.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No corners detected for custom descriptor matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590bf29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Corner Detection**: Harris, Shi-Tomasi, and FAST corner detectors\n",
    "2. **Keypoint Detection**: SIFT, ORB, and AKAZE feature detectors\n",
    "3. **Contour Detection**: Finding and analyzing object boundaries and shapes\n",
    "4. **Feature Matching**: Matching features between images using various algorithms\n",
    "5. **Practical Applications**: Template matching, object tracking, and homography estimation\n",
    "6. **Advanced Techniques**: Multi-scale detection, stability analysis, and custom descriptors\n",
    "\n",
    "### Key Takeaways:\n",
    "- Different feature detectors are suitable for different applications\n",
    "- Corner detectors find intersection points of edges\n",
    "- Keypoint detectors find distinctive points with descriptors for matching\n",
    "- Contour analysis provides shape and geometric information\n",
    "- Feature matching enables object recognition and tracking\n",
    "- RANSAC improves robustness in feature matching\n",
    "- Custom descriptors can be designed for specific applications\n",
    "\n",
    "### Best Practices:\n",
    "- Choose detectors based on your specific requirements (speed vs. accuracy)\n",
    "- Use RANSAC for robust geometric estimation\n",
    "- Filter matches based on distance thresholds\n",
    "- Consider multi-scale approaches for better feature detection\n",
    "- Analyze feature stability for your specific transformations\n",
    "- Combine multiple feature types for better performance\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different detector parameters\n",
    "- Try feature detection on your own images\n",
    "- Implement real-time feature tracking\n",
    "- Explore deep learning-based feature detectors\n",
    "- Apply these techniques to specific computer vision tasks\n",
    "- Combine feature detection with machine learning for object recognition"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
