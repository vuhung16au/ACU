{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca568e4",
   "metadata": {},
   "source": [
    "# Practical Applications of OpenCV\n",
    "\n",
    "This notebook demonstrates real-world applications of OpenCV techniques. We'll explore several practical use cases that combine multiple OpenCV concepts to solve common computer vision problems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Image Quality Assessment](#image-quality-assessment)\n",
    "2. [Document Scanner](#document-scanner)\n",
    "3. [Object Detection and Tracking](#object-detection-and-tracking)\n",
    "4. [Face Detection and Recognition](#face-detection-and-recognition)\n",
    "5. [Barcode and QR Code Detection](#barcode-and-qr-code-detection)\n",
    "6. [Motion Detection](#motion-detection)\n",
    "7. [Image Stitching](#image-stitching)\n",
    "8. [Color-based Object Tracking](#color-based-object-tracking)\n",
    "\n",
    "Let's start by importing all necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6095eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from basic_operations import image_io, display, basic_transforms\n",
    "from filtering import edge_detection, noise_reduction, smoothing\n",
    "from transformations import affine_transforms, perspective_transforms\n",
    "from feature_detection import corner_detection, contour_detection, keypoint_detection\n",
    "from color_processing import color_spaces, histogram, color_enhancement\n",
    "from morphological import basic_morphology, advanced_morphology\n",
    "from advanced import template_matching, image_segmentation\n",
    "from utils import image_utils, visualization\n",
    "\n",
    "# Set up matplotlib for better display\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for this notebook\n",
    "def load_sample_image(filename: str = 'demo_image.jpg') -> np.ndarray:\n",
    "    \"\"\"Load a sample image from the sample_images directory.\"\"\"\n",
    "    image_path = f'../sample_images/original/{filename}'\n",
    "    return image_io.load_image(image_path)\n",
    "\n",
    "def display_results(images: List[np.ndarray], titles: List[str], \n",
    "                   figsize: Tuple[int, int] = (15, 5)) -> None:\n",
    "    \"\"\"Display multiple images in a row.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_image_quality_metrics(image: np.ndarray) -> dict:\n",
    "    \"\"\"Calculate various image quality metrics.\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # Calculate metrics\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()  # Focus measure\n",
    "    mean_brightness = np.mean(gray)\n",
    "    std_contrast = np.std(gray)\n",
    "    \n",
    "    return {\n",
    "        'focus_measure': laplacian_var,\n",
    "        'brightness': mean_brightness,\n",
    "        'contrast': std_contrast\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0125676",
   "metadata": {},
   "source": [
    "## 1. Image Quality Assessment\n",
    "\n",
    "Image quality assessment is crucial in many applications. We'll implement an automated system to evaluate image quality based on multiple metrics including sharpness, brightness, and contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze image quality\n",
    "original_image = load_sample_image()\n",
    "\n",
    "# Create different quality versions for comparison\n",
    "def create_quality_variants(image):\n",
    "    \"\"\"Create different quality versions of an image.\"\"\"\n",
    "    # Blur version (poor focus)\n",
    "    blurred = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "    \n",
    "    # Dark version (poor brightness)\n",
    "    dark = cv2.convertScaleAbs(image, alpha=0.5, beta=-50)\n",
    "    \n",
    "    # Low contrast version\n",
    "    low_contrast = cv2.convertScaleAbs(image, alpha=0.7, beta=0)\n",
    "    \n",
    "    # Noisy version\n",
    "    noise = np.random.normal(0, 25, image.shape).astype(np.uint8)\n",
    "    noisy = cv2.add(image, noise)\n",
    "    \n",
    "    return {\n",
    "        'original': image,\n",
    "        'blurred': blurred,\n",
    "        'dark': dark,\n",
    "        'low_contrast': low_contrast,\n",
    "        'noisy': noisy\n",
    "    }\n",
    "\n",
    "# Create variants\n",
    "variants = create_quality_variants(original_image)\n",
    "\n",
    "# Analyze quality metrics\n",
    "print(\"Image Quality Assessment Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, img in variants.items():\n",
    "    metrics = calculate_image_quality_metrics(img)\n",
    "    print(f\"{name.capitalize()}:\")\n",
    "    print(f\"  Focus Measure: {metrics['focus_measure']:.2f}\")\n",
    "    print(f\"  Brightness: {metrics['brightness']:.2f}\")\n",
    "    print(f\"  Contrast: {metrics['contrast']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Display all variants\n",
    "images = list(variants.values())\n",
    "titles = list(variants.keys())\n",
    "display_results(images, titles, figsize=(20, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fe36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a quality scoring system\n",
    "def calculate_quality_score(image: np.ndarray) -> float:\n",
    "    \"\"\"Calculate an overall quality score (0-100).\"\"\"\n",
    "    metrics = calculate_image_quality_metrics(image)\n",
    "    \n",
    "    # Normalize metrics (these thresholds would be calibrated with more data)\n",
    "    focus_score = min(100, max(0, (metrics['focus_measure'] / 1000) * 100))\n",
    "    brightness_score = 100 - abs(metrics['brightness'] - 128) / 128 * 100\n",
    "    contrast_score = min(100, (metrics['contrast'] / 50) * 100)\n",
    "    \n",
    "    # Weighted average\n",
    "    overall_score = (focus_score * 0.4 + brightness_score * 0.3 + contrast_score * 0.3)\n",
    "    return overall_score\n",
    "\n",
    "# Calculate quality scores for all variants\n",
    "print(\"Quality Scores (0-100):\")\n",
    "print(\"-\" * 30)\n",
    "scores = {}\n",
    "for name, img in variants.items():\n",
    "    score = calculate_quality_score(img)\n",
    "    scores[name] = score\n",
    "    print(f\"{name.capitalize()}: {score:.1f}\")\n",
    "\n",
    "# Visualize quality scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(scores.keys())\n",
    "values = list(scores.values())\n",
    "colors = ['green', 'red', 'orange', 'yellow', 'purple']\n",
    "\n",
    "bars = plt.bar(names, values, color=colors, alpha=0.7)\n",
    "plt.title('Image Quality Scores Comparison')\n",
    "plt.ylabel('Quality Score (0-100)')\n",
    "plt.xlabel('Image Variant')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c5230",
   "metadata": {},
   "source": [
    "## 2. Document Scanner\n",
    "\n",
    "This application demonstrates how to automatically detect and extract documents from images, similar to mobile scanning apps. We'll use edge detection, contour analysis, and perspective transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_like_image(base_image):\n",
    "    \"\"\"Create a document-like image with perspective distortion.\"\"\"\n",
    "    height, width = base_image.shape[:2]\n",
    "    \n",
    "    # Create a white background slightly larger than the base image\n",
    "    background = np.ones((height + 200, width + 200, 3), dtype=np.uint8) * 200\n",
    "    \n",
    "    # Place the image on the background with some offset\n",
    "    offset_y, offset_x = 100, 100\n",
    "    background[offset_y:offset_y+height, offset_x:offset_x+width] = base_image\n",
    "    \n",
    "    # Apply perspective transformation to simulate a document photo\n",
    "    h, w = background.shape[:2]\n",
    "    src_points = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n",
    "    dst_points = np.float32([[50, 20], [w-30, 40], [w-20, h-50], [20, h-30]])\n",
    "    \n",
    "    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    distorted = cv2.warpPerspective(background, matrix, (w, h))\n",
    "    \n",
    "    return distorted\n",
    "\n",
    "def find_document_contour(image):\n",
    "    \"\"\"Find the largest rectangular contour in the image.\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply bilateral filter to reduce noise while preserving edges\n",
    "    filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    \n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(filtered, 50, 150, apertureSize=3)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours by area in descending order\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    \n",
    "    document_contour = None\n",
    "    for contour in contours[:10]:  # Check top 10 largest contours\n",
    "        # Approximate the contour\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        # If the approximated contour has 4 points, it could be a document\n",
    "        if len(approx) == 4:\n",
    "            document_contour = approx\n",
    "            break\n",
    "    \n",
    "    return document_contour, edges\n",
    "\n",
    "# Create a document-like image\n",
    "document_image = create_document_like_image(original_image)\n",
    "\n",
    "# Find document contour\n",
    "doc_contour, edge_image = find_document_contour(document_image)\n",
    "\n",
    "# Visualize the detection process\n",
    "result_image = document_image.copy()\n",
    "if doc_contour is not None:\n",
    "    cv2.drawContours(result_image, [doc_contour], -1, (0, 255, 0), 3)\n",
    "\n",
    "images = [document_image, edge_image, result_image]\n",
    "titles = ['Distorted Document', 'Edge Detection', 'Document Detection']\n",
    "display_results(images, titles, figsize=(18, 6))\n",
    "\n",
    "if doc_contour is not None:\n",
    "    print(\"Document successfully detected!\")\n",
    "    print(f\"Document corners: {doc_contour.reshape(4, 2)}\")\n",
    "else:\n",
    "    print(\"No document found in the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the document perspective if contour was found\n",
    "if doc_contour is not None:\n",
    "    def order_points(pts):\n",
    "        \"\"\"Order points in the order: top-left, top-right, bottom-right, bottom-left.\"\"\"\n",
    "        rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "        \n",
    "        # Top-left point has the smallest sum\n",
    "        # Bottom-right point has the largest sum\n",
    "        s = pts.sum(axis=1)\n",
    "        rect[0] = pts[np.argmin(s)]\n",
    "        rect[2] = pts[np.argmax(s)]\n",
    "        \n",
    "        # Top-right point has the smallest difference\n",
    "        # Bottom-left point has the largest difference\n",
    "        diff = np.diff(pts, axis=1)\n",
    "        rect[1] = pts[np.argmin(diff)]\n",
    "        rect[3] = pts[np.argmax(diff)]\n",
    "        \n",
    "        return rect\n",
    "    \n",
    "    def four_point_transform(image, pts):\n",
    "        \"\"\"Apply perspective transformation to get a top-down view.\"\"\"\n",
    "        rect = order_points(pts)\n",
    "        (tl, tr, br, bl) = rect\n",
    "        \n",
    "        # Compute the width and height of the new image\n",
    "        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "        maxWidth = max(int(widthA), int(widthB))\n",
    "        \n",
    "        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "        maxHeight = max(int(heightA), int(heightB))\n",
    "        \n",
    "        # Destination points for the transform\n",
    "        dst = np.array([\n",
    "            [0, 0],\n",
    "            [maxWidth - 1, 0],\n",
    "            [maxWidth - 1, maxHeight - 1],\n",
    "            [0, maxHeight - 1]], dtype=\"float32\")\n",
    "        \n",
    "        # Compute perspective transform matrix and apply it\n",
    "        M = cv2.getPerspectiveTransform(rect, dst)\n",
    "        warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "        \n",
    "        return warped\n",
    "    \n",
    "    # Extract and correct the document\n",
    "    scanned_document = four_point_transform(document_image, doc_contour.reshape(4, 2))\n",
    "    \n",
    "    # Convert to grayscale and enhance\n",
    "    scanned_gray = cv2.cvtColor(scanned_document, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding for better text readability\n",
    "    scanned_thresh = cv2.adaptiveThreshold(\n",
    "        scanned_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    final_images = [document_image, scanned_document, scanned_gray, scanned_thresh]\n",
    "    final_titles = ['Original', 'Corrected', 'Grayscale', 'Enhanced']\n",
    "    display_results(final_images, final_titles, figsize=(20, 5))\n",
    "    \n",
    "    print(\"Document scanning completed successfully!\")\n",
    "    print(f\"Scanned document size: {scanned_document.shape[:2]}\")\n",
    "else:\n",
    "    print(\"Cannot perform correction - no document detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340a44",
   "metadata": {},
   "source": [
    "## 3. Object Detection and Tracking\n",
    "\n",
    "We'll implement a simple object detection and tracking system using template matching and contour analysis. This is useful for tracking specific objects in video streams or image sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template-based object detection\n",
    "def template_match_detection(image, template, threshold=0.8):\n",
    "    \"\"\"Detect objects using template matching.\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Template matching\n",
    "    result = cv2.matchTemplate(gray_image, gray_template, cv2.TM_CCOEFF_NORMED)\n",
    "    \n",
    "    # Find locations where match exceeds threshold\n",
    "    locations = np.where(result >= threshold)\n",
    "    \n",
    "    # Get template dimensions\n",
    "    h, w = gray_template.shape\n",
    "    \n",
    "    # Draw rectangles around matches\n",
    "    detection_image = image.copy()\n",
    "    for pt in zip(*locations[::-1]):\n",
    "        cv2.rectangle(detection_image, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 2)\n",
    "    \n",
    "    return detection_image, len(locations[0])\n",
    "\n",
    "# Create a template from part of the original image\n",
    "template_size = 100\n",
    "h, w = original_image.shape[:2]\n",
    "template = original_image[h//4:h//4+template_size, w//4:w//4+template_size]\n",
    "\n",
    "# Create a scene with multiple instances of the template\n",
    "scene = np.zeros((h, w*2, 3), dtype=np.uint8)\n",
    "scene[:h, :w] = original_image\n",
    "\n",
    "# Add the template at different locations and scales\n",
    "positions = [(w+50, 50), (w+200, 150), (w+100, 300)]\n",
    "scales = [1.0, 0.8, 1.2]\n",
    "\n",
    "for (x, y), scale in zip(positions, scales):\n",
    "    scaled_template = cv2.resize(template, None, fx=scale, fy=scale)\n",
    "    th, tw = scaled_template.shape[:2]\n",
    "    \n",
    "    # Ensure we don't go out of bounds\n",
    "    end_y = min(y + th, scene.shape[0])\n",
    "    end_x = min(x + tw, scene.shape[1])\n",
    "    actual_th = end_y - y\n",
    "    actual_tw = end_x - x\n",
    "    \n",
    "    if actual_th > 0 and actual_tw > 0:\n",
    "        scene[y:end_y, x:end_x] = scaled_template[:actual_th, :actual_tw]\n",
    "\n",
    "# Perform template matching\n",
    "detected_scene, num_detections = template_match_detection(scene, template, threshold=0.6)\n",
    "\n",
    "# Display results\n",
    "display_results([template, scene, detected_scene], \n",
    "               ['Template', 'Scene', f'Detections ({num_detections})'], \n",
    "               figsize=(18, 6))\n",
    "\n",
    "print(f\"Template matching found {num_detections} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec383b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion detection simulation\n",
    "def simulate_motion_sequence(base_image, num_frames=5):\n",
    "    \"\"\"Simulate a sequence of frames with moving objects.\"\"\"\n",
    "    frames = []\n",
    "    h, w = base_image.shape[:2]\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        frame = base_image.copy()\n",
    "        \n",
    "        # Add a moving circle\n",
    "        center_x = 50 + i * 30\n",
    "        center_y = 100 + i * 10\n",
    "        cv2.circle(frame, (center_x, center_y), 20, (0, 0, 255), -1)\n",
    "        \n",
    "        # Add some noise to simulate real camera conditions\n",
    "        noise = np.random.normal(0, 5, frame.shape).astype(np.int16)\n",
    "        frame = np.clip(frame.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def detect_motion(frame1, frame2, threshold=30):\n",
    "    \"\"\"Detect motion between two frames.\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate absolute difference\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    \n",
    "    # Apply threshold\n",
    "    _, motion_mask = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Remove noise with morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_OPEN, kernel)\n",
    "    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Find contours of moving objects\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding boxes around moving objects\n",
    "    motion_result = frame2.copy()\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 100:  # Filter small movements\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(motion_result, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(motion_result, 'Motion', (x, y - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    return motion_result, motion_mask\n",
    "\n",
    "# Generate motion sequence\n",
    "frames = simulate_motion_sequence(original_image)\n",
    "\n",
    "# Detect motion between consecutive frames\n",
    "motion_results = []\n",
    "motion_masks = []\n",
    "\n",
    "for i in range(len(frames) - 1):\n",
    "    motion_frame, motion_mask = detect_motion(frames[i], frames[i + 1])\n",
    "    motion_results.append(motion_frame)\n",
    "    motion_masks.append(motion_mask)\n",
    "\n",
    "# Display a few examples\n",
    "if len(motion_results) >= 2:\n",
    "    display_results([frames[0], frames[1], motion_results[0], motion_masks[0]], \n",
    "                   ['Frame 1', 'Frame 2', 'Motion Detection', 'Motion Mask'], \n",
    "                   figsize=(20, 5))\n",
    "    \n",
    "    print(f\"Motion detection completed for {len(motion_results)} frame pairs\")\n",
    "else:\n",
    "    print(\"Not enough frames for motion detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4771d35",
   "metadata": {},
   "source": [
    "## 4. Face Detection and Recognition\n",
    "\n",
    "We'll implement face detection using OpenCV's built-in Haar cascades. This is a classic computer vision application with many practical uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba08652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face detection implementation\n",
    "def detect_faces_haar(image):\n",
    "    \"\"\"Detect faces using Haar cascades.\"\"\"\n",
    "    try:\n",
    "        # Load the face cascade classifier\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "        # Draw rectangles around faces\n",
    "        result_image = image.copy()\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(result_image, f'Face {len(faces)}', (x, y - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        \n",
    "        return result_image, len(faces)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Face detection error: {e}\")\n",
    "        return image, 0\n",
    "\n",
    "def create_synthetic_face_like_regions(image):\n",
    "    \"\"\"Create synthetic face-like regions for demonstration.\"\"\"\n",
    "    result = image.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Create some oval regions that might be detected as faces\n",
    "    face_regions = [\n",
    "        (w//4, h//4, w//6, h//8),  # x, y, width, height\n",
    "        (w//2, h//3, w//8, h//10),\n",
    "        (3*w//4, h//2, w//7, h//9)\n",
    "    ]\n",
    "    \n",
    "    for i, (x, y, fw, fh) in enumerate(face_regions):\n",
    "        # Draw an oval\n",
    "        cv2.ellipse(result, (x + fw//2, y + fh//2), (fw//2, fh//2), 0, 0, 360, (0, 255, 255), 2)\n",
    "        cv2.putText(result, f'Region {i+1}', (x, y - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)\n",
    "    \n",
    "    return result, len(face_regions)\n",
    "\n",
    "# Try face detection on the original image\n",
    "face_detected_image, num_faces = detect_faces_haar(original_image)\n",
    "\n",
    "# Create synthetic face-like regions for demonstration\n",
    "synthetic_face_image, num_regions = create_synthetic_face_like_regions(original_image)\n",
    "\n",
    "# Display results\n",
    "if num_faces > 0:\n",
    "    display_results([original_image, face_detected_image], \n",
    "                   ['Original', f'Faces Detected: {num_faces}'], \n",
    "                   figsize=(12, 6))\n",
    "    print(f\"Successfully detected {num_faces} faces!\")\n",
    "else:\n",
    "    display_results([original_image, synthetic_face_image], \n",
    "                   ['Original', f'Synthetic Face Regions: {num_regions}'], \n",
    "                   figsize=(12, 6))\n",
    "    print(f\"No faces detected in the original image. Showing {num_regions} synthetic regions for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55134293",
   "metadata": {},
   "source": [
    "## 5. Color-based Object Tracking\n",
    "\n",
    "This application demonstrates how to track objects based on their color. This is useful for tracking colored balls, markers, or any objects with distinctive colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color-based object tracking\n",
    "def create_color_tracking_scene(base_image):\n",
    "    \"\"\"Create a scene with colored objects to track.\"\"\"\n",
    "    scene = base_image.copy()\n",
    "    h, w = scene.shape[:2]\n",
    "    \n",
    "    # Add colored circles\n",
    "    colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (255, 255, 0), (255, 0, 255)]  # BGR format\n",
    "    positions = [(100, 100), (300, 150), (500, 200), (200, 300), (400, 350)]\n",
    "    \n",
    "    for color, pos in zip(colors, positions):\n",
    "        if pos[1] < h and pos[0] < w:  # Make sure we're within image bounds\n",
    "            cv2.circle(scene, pos, 30, color, -1)\n",
    "    \n",
    "    return scene\n",
    "\n",
    "def track_color(image, target_color, tolerance=30):\n",
    "    \"\"\"Track objects of a specific color.\"\"\"\n",
    "    # Convert BGR to HSV for better color segmentation\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Convert target color to HSV\n",
    "    target_bgr = np.uint8([[target_color]])\n",
    "    target_hsv = cv2.cvtColor(target_bgr, cv2.COLOR_BGR2HSV)[0][0]\n",
    "    \n",
    "    # Define color range\n",
    "    lower_bound = np.array([max(0, target_hsv[0] - tolerance), 50, 50])\n",
    "    upper_bound = np.array([min(179, target_hsv[0] + tolerance), 255, 255])\\n    \n",
    "    # Create color mask\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "    \n",
    "    # Remove noise\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Draw bounding boxes around detected objects\n",
    "    result = image.copy()\n",
    "    tracked_objects = 0\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 500:  # Filter small areas\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(result, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Calculate center\n",
    "            center_x = x + w // 2\n",
    "            center_y = y + h // 2\n",
    "            cv2.circle(result, (center_x, center_y), 5, (0, 255, 0), -1)\n",
    "            \n",
    "            cv2.putText(result, f'Object {tracked_objects + 1}', (x, y - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            tracked_objects += 1\n",
    "    \n",
    "    return result, mask, tracked_objects\n",
    "\n",
    "# Create a scene with colored objects\n",
    "colored_scene = create_color_tracking_scene(original_image)\n",
    "\n",
    "# Track different colors\n",
    "colors_to_track = [\n",
    "    ((0, 0, 255), \"Red\"),    # BGR format\n",
    "    ((0, 255, 0), \"Green\"),\n",
    "    ((255, 0, 0), \"Blue\")\n",
    "]\n",
    "\n",
    "tracking_results = []\n",
    "for color_bgr, color_name in colors_to_track:\n",
    "    tracked_image, color_mask, num_objects = track_color(colored_scene, color_bgr, tolerance=40)\n",
    "    tracking_results.append((tracked_image, color_mask, num_objects, color_name))\n",
    "\n",
    "# Display original scene and first tracking result\n",
    "if tracking_results:\n",
    "    first_result = tracking_results[0]\n",
    "    display_results([colored_scene, first_result[0], first_result[1]], \n",
    "                   ['Scene with Colored Objects', f'{first_result[3]} Tracking', f'{first_result[3]} Mask'], \n",
    "                   figsize=(18, 6))\n",
    "    \n",
    "    print(\"Color Tracking Results:\")\n",
    "    for _, _, num_objects, color_name in tracking_results:\n",
    "        print(f\"  {color_name}: {num_objects} objects detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fa46f",
   "metadata": {},
   "source": [
    "## 6. Image Stitching\n",
    "\n",
    "Image stitching is the process of combining multiple overlapping images to create a panoramic image. This is commonly used in photography and mapping applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08748325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image stitching implementation\n",
    "def create_overlapping_images(base_image):\n",
    "    \"\"\"Create overlapping images to simulate panorama source images.\"\"\"\n",
    "    h, w = base_image.shape[:2]\n",
    "    overlap_ratio = 0.3  # 30% overlap\n",
    "    \n",
    "    # Create three overlapping sections\n",
    "    section_width = int(w * 0.7)  # Each section is 70% of original width\n",
    "    overlap_pixels = int(w * overlap_ratio)\n",
    "    \n",
    "    # Left image\n",
    "    left_img = base_image[:, :section_width]\n",
    "    \n",
    "    # Middle image (shifted right)\n",
    "    start_x = section_width - overlap_pixels\n",
    "    end_x = start_x + section_width\n",
    "    if end_x > w:\n",
    "        end_x = w\n",
    "        start_x = end_x - section_width\n",
    "    middle_img = base_image[:, start_x:end_x]\n",
    "    \n",
    "    # Right image (shifted further right)\n",
    "    start_x = min(w - section_width, start_x + section_width - overlap_pixels)\n",
    "    right_img = base_image[:, start_x:]\n",
    "    \n",
    "    return [left_img, middle_img, right_img]\n",
    "\n",
    "def simple_image_stitching(images):\n",
    "    \"\"\"Perform simple image stitching using OpenCV's Stitcher.\"\"\"\n",
    "    try:\n",
    "        # Create stitcher object\n",
    "        stitcher = cv2.Stitcher.create()\n",
    "        \n",
    "        # Perform stitching\n",
    "        status, stitched = stitcher.stitch(images)\n",
    "        \n",
    "        if status == cv2.Stitcher_OK:\n",
    "            return stitched, True\n",
    "        else:\n",
    "            print(f\"Stitching failed with status: {status}\")\n",
    "            return None, False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Stitching error: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def manual_feature_matching(img1, img2):\n",
    "    \"\"\"Manual feature matching and stitching (simplified version).\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect SIFT features\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "    \n",
    "    if des1 is None or des2 is None:\n",
    "        return None, 0\n",
    "    \n",
    "    # Match features\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for match_pair in matches:\n",
    "        if len(match_pair) == 2:\n",
    "            m, n = match_pair\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                good_matches.append(m)\n",
    "    \n",
    "    # Draw matches\n",
    "    matched_image = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \n",
    "                                   flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    return matched_image, len(good_matches)\n",
    "\n",
    "# Create overlapping images\n",
    "overlapping_images = create_overlapping_images(original_image)\n",
    "\n",
    "# Display the source images\n",
    "display_results(overlapping_images, ['Left', 'Middle', 'Right'], figsize=(18, 6))\n",
    "\n",
    "# Try stitching with OpenCV's built-in stitcher\n",
    "stitched_result, success = simple_image_stitching(overlapping_images)\n",
    "\n",
    "if success and stitched_result is not None:\n",
    "    # Display stitching result\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(cv2.cvtColor(stitched_result, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Stitched Panorama')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Image stitching completed successfully!\")\n",
    "    print(f\"Original size: {original_image.shape[:2]}\")\n",
    "    print(f\"Stitched size: {stitched_result.shape[:2]}\")\n",
    "else:\n",
    "    print(\"Built-in stitching failed. Trying manual feature matching...\")\n",
    "    \n",
    "    # Try manual feature matching between first two images\n",
    "    if len(overlapping_images) >= 2:\n",
    "        matched_img, num_matches = manual_feature_matching(overlapping_images[0], overlapping_images[1])\n",
    "        \n",
    "        if matched_img is not None:\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(cv2.cvtColor(matched_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f'Feature Matching ({num_matches} good matches)')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Feature matching found {num_matches} good matches\")\n",
    "        else:\n",
    "            print(\"Feature matching also failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa451c5",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis and Optimization\n",
    "\n",
    "Let's analyze the performance of different techniques and discuss optimization strategies for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis of different techniques\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "def benchmark_function(func: Callable, *args, iterations: int = 10, **kwargs) -> dict:\n",
    "    \"\"\"Benchmark a function's performance.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# Benchmark different operations\n",
    "print(\"Performance Benchmarking Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test image processing operations\n",
    "test_image = original_image\n",
    "\n",
    "# 1. Basic operations\n",
    "def test_resize():\n",
    "    return cv2.resize(test_image, (400, 300))\n",
    "\n",
    "def test_blur():\n",
    "    return cv2.GaussianBlur(test_image, (15, 15), 0)\n",
    "\n",
    "def test_edge_detection():\n",
    "    gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Canny(gray, 50, 150)\n",
    "\n",
    "def test_color_conversion():\n",
    "    return cv2.cvtColor(test_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Benchmark operations\n",
    "operations = [\n",
    "    (\"Resize\", test_resize),\n",
    "    (\"Gaussian Blur\", test_blur),\n",
    "    (\"Edge Detection\", test_edge_detection),\n",
    "    (\"Color Conversion\", test_color_conversion)\n",
    "]\n",
    "\n",
    "benchmark_results = {}\n",
    "for name, func in operations:\n",
    "    result = benchmark_function(func, iterations=20)\n",
    "    benchmark_results[name] = result\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean time: {result['mean_time']*1000:.2f} ms\")\n",
    "    print(f\"  Std dev: {result['std_time']*1000:.2f} ms\")\n",
    "    print(f\"  Min time: {result['min_time']*1000:.2f} ms\")\n",
    "    print(f\"  Max time: {result['max_time']*1000:.2f} ms\")\n",
    "    print()\n",
    "\n",
    "# Visualize performance comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Extract data for plotting\n",
    "names = list(benchmark_results.keys())\n",
    "mean_times = [benchmark_results[name]['mean_time'] * 1000 for name in names]  # Convert to ms\n",
    "std_times = [benchmark_results[name]['std_time'] * 1000 for name in names]\n",
    "\n",
    "# Create bar plot with error bars\n",
    "bars = plt.bar(names, mean_times, yerr=std_times, capsize=5, alpha=0.7, color=['blue', 'green', 'red', 'orange'])\n",
    "\n",
    "plt.title('Performance Comparison of OpenCV Operations')\n",
    "plt.ylabel('Execution Time (milliseconds)')\n",
    "plt.xlabel('Operation')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean_time in zip(bars, mean_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{mean_time:.1f}ms', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage analysis and optimization strategies\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "def analyze_memory_usage(func, *args, **kwargs):\n",
    "    \"\"\"Analyze memory usage of a function.\"\"\"\n",
    "    initial_memory = get_memory_usage()\n",
    "    result = func(*args, **kwargs)\n",
    "    final_memory = get_memory_usage()\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'initial_memory': initial_memory,\n",
    "        'final_memory': final_memory,\n",
    "        'memory_increase': final_memory - initial_memory\n",
    "    }\n",
    "\n",
    "# Test memory usage for different image sizes\n",
    "def create_large_image(scale_factor):\n",
    "    \"\"\"Create a large image by scaling the original.\"\"\"\n",
    "    return cv2.resize(original_image, None, fx=scale_factor, fy=scale_factor)\n",
    "\n",
    "print(\"Memory Usage Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test different image sizes\n",
    "scale_factors = [1.0, 2.0, 3.0, 4.0]\n",
    "memory_results = []\n",
    "\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "print()\n",
    "\n",
    "for scale in scale_factors:\n",
    "    def test_large_image():\n",
    "        large_img = create_large_image(scale)\n",
    "        # Perform some operations on the large image\n",
    "        gray = cv2.cvtColor(large_img, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "        edges = cv2.Canny(blurred, 50, 150)\n",
    "        return large_img, gray, blurred, edges\n",
    "    \n",
    "    memory_analysis = analyze_memory_usage(test_large_image)\n",
    "    memory_results.append((scale, memory_analysis['memory_increase']))\n",
    "    \n",
    "    print(f\"Scale factor {scale}x:\")\n",
    "    print(f\"  Memory increase: {memory_analysis['memory_increase']:.2f} MB\")\n",
    "    print(f\"  Image size: {memory_analysis['result'][0].shape}\")\n",
    "    print()\n",
    "\n",
    "# Visualize memory usage\n",
    "plt.figure(figsize=(10, 6))\n",
    "scales = [result[0] for result in memory_results]\n",
    "memory_increases = [result[1] for result in memory_results]\n",
    "\n",
    "plt.plot(scales, memory_increases, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('Memory Usage vs Image Scale Factor')\n",
    "plt.xlabel('Scale Factor')\n",
    "plt.ylabel('Memory Increase (MB)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for scale, memory in zip(scales, memory_increases):\n",
    "    plt.annotate(f'{memory:.1f} MB', (scale, memory), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optimization tips\n",
    "print(\"\\nOptimization Strategies for Real-world Applications:\")\n",
    "print(\"=\" * 55)\n",
    "print(\"1. **Image Preprocessing:**\")\n",
    "print(\"   - Resize images to appropriate size before processing\")\n",
    "print(\"   - Use appropriate data types (uint8 vs float32)\")\n",
    "print(\"   - Consider ROI (Region of Interest) processing\")\n",
    "print()\n",
    "print(\"2. **Algorithm Selection:**\")\n",
    "print(\"   - Choose simpler algorithms when accuracy allows\")\n",
    "print(\"   - Use approximation methods for real-time applications\")\n",
    "print(\"   - Consider hardware acceleration (GPU, specialized chips)\")\n",
    "print()\n",
    "print(\"3. **Memory Management:**\")\n",
    "print(\"   - Release large arrays when no longer needed\")\n",
    "print(\"   - Use in-place operations when possible\")\n",
    "print(\"   - Process images in chunks for very large datasets\")\n",
    "print()\n",
    "print(\"4. **Parallel Processing:**\")\n",
    "print(\"   - Use multi-threading for independent operations\")\n",
    "print(\"   - Leverage OpenCV's parallel processing capabilities\")\n",
    "print(\"   - Consider distributed processing for large-scale applications\")\n",
    "print()\n",
    "print(\"5. **Caching and Precomputation:**\")\n",
    "print(\"   - Cache frequently used computations\")\n",
    "print(\"   - Precompute expensive operations when possible\")\n",
    "print(\"   - Use lookup tables for complex functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3582e74",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "This notebook has demonstrated several practical applications of OpenCV, showcasing how individual techniques can be combined to solve real-world computer vision problems.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Image Quality Assessment**: Automated evaluation of image properties is crucial for many applications\n",
    "2. **Document Scanner**: Combining edge detection, contour analysis, and perspective transformation creates powerful document processing tools\n",
    "3. **Object Detection and Tracking**: Template matching and motion detection form the basis of many surveillance and automation systems\n",
    "4. **Face Detection**: Pre-trained classifiers can quickly identify faces for security and user interface applications\n",
    "5. **Color-based Tracking**: Simple yet effective method for tracking colored objects in controlled environments\n",
    "6. **Image Stitching**: Panoramic image creation requires feature detection, matching, and geometric transformations\n",
    "7. **Performance Optimization**: Understanding computational and memory requirements is essential for production systems\n",
    "\n",
    "### Real-world Applications:\n",
    "\n",
    "- **Security Systems**: Motion detection, face recognition, object tracking\n",
    "- **Medical Imaging**: Quality assessment, automated measurements, anomaly detection\n",
    "- **Industrial Automation**: Defect detection, robotic vision, quality control\n",
    "- **Mobile Apps**: Document scanning, augmented reality, photo enhancement\n",
    "- **Autonomous Vehicles**: Object detection, lane detection, obstacle avoidance\n",
    "\n",
    "### Next Steps for Further Learning:\n",
    "\n",
    "1. **Deep Learning Integration**: Explore YOLO, SSD, and other neural network-based detectors\n",
    "2. **Real-time Processing**: Implement video stream processing and optimization\n",
    "3. **3D Vision**: Study stereo vision, depth estimation, and 3D reconstruction\n",
    "4. **Advanced Tracking**: Implement Kalman filters and particle filters for robust tracking\n",
    "5. **Custom Applications**: Develop domain-specific solutions for your particular use case\n",
    "\n",
    "### Resources for Continued Learning:\n",
    "\n",
    "- OpenCV Documentation: https://docs.opencv.org/\n",
    "- Computer Vision courses and tutorials\n",
    "- Research papers on specific techniques\n",
    "- Open-source computer vision projects\n",
    "- Industry-specific computer vision applications\n",
    "\n",
    "Remember that the key to mastering computer vision is practice with real datasets and understanding the trade-offs between accuracy, speed, and computational resources in your specific application domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98583b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of all techniques demonstrated\n",
    "print(\"ðŸŽ‰ Practical Applications Notebook Complete! ðŸŽ‰\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTechniques Covered:\")\n",
    "print(\"âœ“ Image Quality Assessment\")\n",
    "print(\"âœ“ Document Scanner\")\n",
    "print(\"âœ“ Object Detection and Tracking\")\n",
    "print(\"âœ“ Face Detection\")\n",
    "print(\"âœ“ Color-based Object Tracking\")\n",
    "print(\"âœ“ Image Stitching\")\n",
    "print(\"âœ“ Performance Analysis\")\n",
    "print(\"âœ“ Memory Optimization\")\n",
    "\n",
    "print(f\"\\nTotal execution time: {time.time() - time.time():.2f} seconds\")\n",
    "print(\"Notebook ready for production use and further experimentation!\")\n",
    "\n",
    "# Cleanup large variables to free memory\n",
    "try:\n",
    "    del variants, tracking_results, overlapping_images\n",
    "    if 'stitched_result' in locals():\n",
    "        del stitched_result\n",
    "    print(\"\\nâœ“ Memory cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
